{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8121977c",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699b8529",
   "metadata": {},
   "source": [
    "This notebook is intended to serve as a guide to train custom Multiplexer models. The following sections demonstrate how the BelugaMultiplexer model was trained and includes starter code to generate data, define model parameters, and perform back-propogation. Additionally, since Models may have varying input and output dimensionality, many parameters are left empty for users to fill-in according to the size of their own model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7f6440",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26296de4",
   "metadata": {},
   "source": [
    "For dimensions to match the pre-written code in both the training notebook and the command line interface tool, the user models must match the following dimension format: \n",
    "\n",
    "For the user's **Base Model**: \\\n",
    "--inputs must be of shape `[batch_size, 4, sequence_length]` \\\n",
    "--the output must be of shape `[batch_size, predicted_features]` \n",
    "\n",
    "If the user's base model is already trained and does not match the this input/output format (for example, it may include an extra dimension), it is recommended that users adjust the dimensions of the input/output in the forward method of their model.\n",
    "\n",
    "For the user's **Multiplexer Model**: \\\n",
    "--inputs must be in the shape `[batch_size, 4, sequence_length]` \\\n",
    "--the output must be in the shape `[batch_size, predicted_features, 4, sequence_length]` \n",
    "\n",
    "where: \\\n",
    "--`batch_size` is the number of sequence in the same tensor. \\\n",
    "--`sequence_length` is the number base-pairs in a sequence. \\\n",
    "--`predicted_features` is the number of predictions made for 1 sequence by the base model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa523b5",
   "metadata": {},
   "source": [
    "#### 1A) Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584f98cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pyfasta\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../models')\n",
    "from BelugaMultiplexer import BelugaMultiplexer\n",
    "from Beluga import Beluga"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef3562",
   "metadata": {},
   "source": [
    "#### 1B) Generate Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b692ebaa",
   "metadata": {},
   "source": [
    "The following methods were used to generate training data for Beluga Multiplexer. `all_mutations` generated a random 2000 base-pair reference sequence as well as a set of alternative sequences that represent every positional mutation. Each base-pair was 1-hot encoded (e.g 'A' = [1,0,0,0]) such that a 4x2000 dimensioned tensor represents the 2,000 base-pair sequence and a 8000x4x2000 dimensioned tensor represents the alternative mutations. Note that the length of a sequence is an adjustable parameter but was set to 2000 for Beluga Multiplexer training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ad395a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_mutations(pos, chrome_num, length):\n",
    "    \"\"\"\n",
    "    returns an encoded sequence and every possible mutation\n",
    "    \n",
    "    Args:\n",
    "        pos : int\n",
    "            Center of the sequence\n",
    "        \n",
    "        chrome_num : string\n",
    "            The chromosome the sequence is sampled from\n",
    "        \n",
    "        length : int\n",
    "            The number of base-pairs sampled\n",
    "    \n",
    "    Returns:\n",
    "        encoded_ref: (4,length) sized encoding of the sequence drawn from 'chrome_num' centered at 'pos'\n",
    "        \n",
    "        mutations: (4*length, 4, length) sized encoding representing all possible mutations of the reference allele\n",
    "\n",
    "    \"\"\"\n",
    "    \n",
    "    if length % 2 == 0:\n",
    "        lower = int(length/2) - 1\n",
    "        upper = int(length/2)\n",
    "        \n",
    "    else:\n",
    "        lower = int(length/2)\n",
    "        upper = int(length/2)\n",
    "    \n",
    "    seq = genome.sequence({'chr': chrome_num, 'start': pos - lower , 'stop': pos + upper})\n",
    "\n",
    "\n",
    "    #encode the sequence\n",
    "    mydict = {'A': torch.tensor([1, 0, 0, 0]), 'G': torch.tensor([0, 1, 0, 0]),\n",
    "            'C': torch.tensor([0, 0, 1, 0]), 'T': torch.tensor([0, 0, 0, 1]),\n",
    "            'N': torch.tensor([0, 0, 0, 0]), 'H': torch.tensor([0, 0, 0, 0]),\n",
    "            'a': torch.tensor([1, 0, 0, 0]), 'g': torch.tensor([0, 1, 0, 0]),\n",
    "            'c': torch.tensor([0, 0, 1, 0]), 't': torch.tensor([0, 0, 0, 1]),\n",
    "            'n': torch.tensor([0, 0, 0, 0]), '-': torch.tensor([0, 0, 0, 0])}\n",
    "    \n",
    "    \n",
    "    #this dictionary returns a list of possible mutations for each nucleotide\n",
    "    mutation_dict = {'a': ['a','g', 'c', 't'], 'A':['a','g', 'c', 't'],\n",
    "                    'c': ['a','g', 'c', 't'], 'C':['a','g', 'c', 't'],\n",
    "                    'g': ['a','g', 'c', 't'], 'G':['a','g', 'c', 't'],\n",
    "                    't': ['a','g', 'c', 't'], 'T':['a','g', 'c', 't'],\n",
    "                    'n': ['n', 'n', 'n', 'n'], 'N':['n', 'n', 'n', 'n'],\n",
    "                    '-': ['n', 'n', 'n', 'n']}\n",
    "    \n",
    "    #each column is the encoding for each nucleotide in the original seq\n",
    "    encoded_ref = torch.zeros((4, len(seq)))\n",
    "    for i in range(len(seq)):\n",
    "        #this implements the encoding\n",
    "        encoded_ref[:,i] = mydict[seq[i]]\n",
    "\n",
    "    \n",
    "    mutations = torch.tile(encoded_ref, (length*4, 1, 1)) \n",
    "    \n",
    "    for j in range(len(seq)):\n",
    "        #for each element in the original sequence, create 4 \"mutation layers\" \n",
    "        i = j*4\n",
    "        mutations[i, :, j] = mydict[mutation_dict[seq[j]][0]]\n",
    "        mutations[i + 1, :, j] = mydict[mutation_dict[seq[j]][1]]\n",
    "        mutations[i + 2, :, j] = mydict[mutation_dict[seq[j]][2]]\n",
    "        mutations[i + 3, :, j] = mydict[mutation_dict[seq[j]][3]]\n",
    "        \n",
    "        \n",
    "    return encoded_ref, mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a854819",
   "metadata": {},
   "source": [
    "The training data was then generated by taking both the reference and alternative sequences and passing them through the Beluga model in batches. We then calculated a tensor that represented the log-fold difference between these predictions, and used it as our training target: given a reference, we wanted to train the BelugaMultiplexer model to predict the log-fold difference between the reference and all possible alternative predictions made by Beluga.\n",
    "\n",
    "The methods `training_data` generates training data from every chromosomes but 'chr8' and 'chr9' while `validation_data` only draws DNA sequences from chromosomes 'chr8' and 'chr9'.\n",
    "\n",
    "The methods `training_data` and `validation_data` create the data while `gen_training_data` and `gen_validation_data` enable users to specify how many sets of training data to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73ed744c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_fold(alt, ref):\n",
    "    \"\"\"\n",
    "    Returns the log fold change of alt, ref\n",
    "    \n",
    "    equals: log(((alt+1e-6) * (1-ref+1e-6)) /((1-alt+1e-6) * (ref+1e-6)) \n",
    "    \"\"\"\n",
    "    \n",
    "    e = 10**(-6)\n",
    "    top = (alt + e)*(1 - ref + e)\n",
    "    bot = (1 - alt + e) * (ref + e)\n",
    "    \n",
    "    return torch.log(top/bot)\n",
    "\n",
    "\n",
    "    \n",
    "def training_data(length, training_CHRS, model, batch_size = 64, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    generates 1 training sample. The input is a randomly generated chromosome and the target is a set \n",
    "    of model_output_dim predictions. \n",
    "    \n",
    "    Args:\n",
    "        length : int\n",
    "            The number of base-pairs in one generated sequence\n",
    "        \n",
    "        training_CHRS : array\n",
    "            Set of chromosomes to be considered for training\n",
    "        \n",
    "        model : Neural Network Object\n",
    "            The model that makes the predictions\n",
    "        \n",
    "        batch_size : int\n",
    "            Size of batch that model takes in\n",
    "        \n",
    "        device: 'cpu' or 'cuda'\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "        1 training sample (input, target)\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    model = model\n",
    "    model.eval()\n",
    "    \n",
    "\n",
    "\n",
    "    size = []    \n",
    "    for i in training_CHRS:\n",
    "        chr_length = len(genome[i])\n",
    "        size.append(chr_length) \n",
    "        size_normalized = size/np.sum(size)\n",
    "    \n",
    "    \n",
    "    #Sample chromosome with probability proportional its length\n",
    "    training_chromosome = np.random.choice(training_CHRS, p = size_normalized)\n",
    "\n",
    "\n",
    "    #Reject samples with too many 'N' values\n",
    "    N_count = 11\n",
    "    while N_count > 10:\n",
    "        if length % 2 == 0:\n",
    "            lower = int(length/2) - 1\n",
    "            upper = int(length/2)\n",
    "        \n",
    "        else:\n",
    "            lower = int(length/2)\n",
    "            upper = int(length/2)\n",
    "    \n",
    "        pos = np.random.randint(lower, len(genome[training_chromosome]) - upper)\n",
    "        seq = genome.sequence({'chr': training_chromosome, 'start': pos - lower , 'stop': pos + upper})\n",
    "        \n",
    "        #checks number of N's in the sequence is less than 10\n",
    "        N_count = seq.count(\"N\") \n",
    "            \n",
    "\n",
    "    ref_arr_encoded, alt_arr_encoded = all_mutations(pos, training_chromosome, length)\n",
    " \n",
    "    with torch.no_grad():\n",
    "        reference_pred = model.forward(ref_arr_encoded.unsqueeze(0).float().to(device))\n",
    "    \n",
    " \n",
    "    alt_pred_arr = []\n",
    "    for i in range(int(math.floor(length*4/batch_size))):\n",
    "        inputs = alt_arr_encoded[i*batch_size : (i+1)*batch_size] \n",
    "        input = inputs.to(device).float()\n",
    "        with torch.no_grad():\n",
    "            alt_pred_arr.append(model.forward(input))        \n",
    "    alt_predictions = torch.vstack(alt_pred_arr)\n",
    "\n",
    "    \n",
    "\n",
    "    return ref_arr_encoded, log_fold(alt_predictions, reference_pred)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def validation_data(length, validation_CHRS,  model, batch_size = 16, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    generates 1 validation sample. The input is a randomly generated chromosome and the target [FIX!!!!!]. \n",
    "    \n",
    "    Args:\n",
    "        length : int\n",
    "            The number of base-pairs in one generated sequence\n",
    "        \n",
    "        validation_CHRS : array\n",
    "            Set of chromosomes to be considered for validation testing\n",
    "        \n",
    "        model : Neural Network Object\n",
    "            The model that makes the predictions\n",
    "        \n",
    "        batch_size : int\n",
    "            Size of batch that model takes in\n",
    "        \n",
    "        device: 'cpu' or 'cuda'\n",
    "    \n",
    "    \n",
    "    Returns: 2 torch.tensor \n",
    "        1 validation sample (input, target)\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    model = model\n",
    "    model.eval()\n",
    " \n",
    "    #Sample chromosome with probability proportional its length\n",
    "    val_probability = [len(genome[i]) for i in validation_CHRS]\n",
    "    val_prob_normalized = val_probability/np.sum(val_probability)\n",
    "    val_chromosome = np.random.choice([\"chr8\", \"chr9\"], p = val_prob_normalized)\n",
    "    \n",
    "    \n",
    "    #Reject samples with too many 'N' values\n",
    "    N_count = 11\n",
    "    while N_count > 10:\n",
    "        if length % 2 == 0:\n",
    "            lower = int(length/2) - 1\n",
    "            upper = int(length/2)\n",
    "        \n",
    "        else:\n",
    "            lower = int(length/2)\n",
    "            upper = int(length/2)\n",
    "    \n",
    "        pos = np.random.randint(lower, len(genome[val_chromosome]) - upper)\n",
    "        seq = genome.sequence({'chr': val_chromosome, 'start': pos - lower , 'stop': pos + upper})\n",
    "        \n",
    "        #checks number of N's in the sequence is less than 10\n",
    "        N_count = seq.count(\"N\")  \n",
    "            \n",
    "    ref_arr_encoded, alt_arr_encoded = all_mutations(pos, val_chromosome, length)\n",
    "    \n",
    "    \n",
    "    ref_input = ref_arr_encoded.unsqueeze(0).to(device).float()\n",
    "    with torch.no_grad():\n",
    "        reference = model.forward(ref_input)\n",
    "\n",
    "    alt_pred_arr = []\n",
    "    for i in range(int(math.floor(length*4/batch_size))):\n",
    "        inputs = alt_arr_encoded[i*batch_size : (i+1)*batch_size] \n",
    "        input = inputs.to(device).float()\n",
    "        with torch.no_grad():\n",
    "            alt_pred_arr.append(model.forward(input))        \n",
    "    alt_predictions = torch.vstack(alt_pred_arr)\n",
    "\n",
    "\n",
    "    return ref_arr_encoded, log_fold( alt_predictions, reference)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c385099f",
   "metadata": {},
   "source": [
    "`gen_training_data` and `gen_validation_data` generate mutliple batches of training data and validation data, respectively. Within these methods, users can specify the number of sequences they want to generate, the length of each sequence, the original model they want to train the mutliplexer on, and the dimension of the original model's output. These methods then each return a tensor that contains `num_seqs` (input, target) pairs that can directly be used to train or validate the multiplexer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d0b7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_training_data(num_seqs, length, training_CHRS, model, model_output_dim, batch_size = 16, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Generates num_seqs # of training samples by calling gen_training_data\n",
    "    \n",
    "    Args:\n",
    "        num_seqs : int\n",
    "            The number of training samples generated by the method\n",
    "        \n",
    "        length : int\n",
    "            The number of base-pairs in one generate sequence\n",
    "        \n",
    "        training_CHRS : array\n",
    "            Set of chromosomes to be considered for training\n",
    "        \n",
    "        model : Neural Network Object\n",
    "            The model that makes the predictions\n",
    "        \n",
    "        model_output_dim: int\n",
    "            The dimension of the output prediction made by the model\n",
    "        \n",
    "        batch_size: int\n",
    "            batch_size used when generating training target\n",
    "        \n",
    "        device: 'cpu' or 'cuda'\n",
    "        \n",
    "    Returns:\n",
    "        training_input_arr: an array of model inputs used as training data\n",
    "        \n",
    "        target_arr: an array of targets used for training data\n",
    "    \n",
    "    \"\"\"\n",
    "    training_input_arr = torch.zeros((num_seqs, 4, length))\n",
    "    target_arr = torch.zeros((num_seqs, length*4, model_output_dim))\n",
    "    for i in range(num_seqs):\n",
    "        training_input, target = training_data(length, training_CHRS, model, batch_size, device)\n",
    "        training_input_arr[i, :, :] = training_input\n",
    "        target_arr[i, :, :] = target\n",
    "        \n",
    "        \n",
    "    return training_input_arr.float(), target_arr.float()\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def gen_validation_data(num_seqs, length, validation_CHRS, model, model_output_dim, batch_size = 16, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Generates num_seqs # of training samples by calling gen_validation_data\n",
    "    \n",
    "    Args:\n",
    "        num_seqs: int\n",
    "            The number of training samples generated by the method\n",
    "        \n",
    "        length : int\n",
    "            The number of base-pairs in one generate sequence\n",
    "        \n",
    "        validation_CHRS : array\n",
    "            Set of chromosomes to be considered for validation\n",
    "        \n",
    "        model : Neural Network Object\n",
    "            The model that makes the predictions\n",
    "        \n",
    "        model_output_dim : int\n",
    "            The dimension of the output prediction made by the model\n",
    "        \n",
    "        batch_size : int\n",
    "            batch_size used when generating training target\n",
    "        \n",
    "        device: 'cpu' or 'cuda'\n",
    "        \n",
    "    Returns:\n",
    "        val_input_arr : torch.tensor\n",
    "            An array of model inputs\n",
    "        \n",
    "        val_target_arr : torch.tensor\n",
    "            An array of labels for the model input data\n",
    "    \n",
    "    \"\"\"\n",
    "    val_input_arr = torch.zeros((num_seqs, 4, length))\n",
    "    val_target_arr = torch.zeros((num_seqs, length*4, model_output_dim))\n",
    "    \n",
    "    for i in range(num_seqs):\n",
    "        val_input, target = validation_data(length, validation_CHRS, model, batch_size, device)\n",
    "        val_input_arr[i, :, :] = val_input\n",
    "        val_target_arr[i, :, :] = target\n",
    "        \n",
    "    return val_input_arr.float(), val_target_arr.float()\n",
    "  \n",
    "\n",
    "\n",
    "class Multiplexer_Data(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.length = x.shape[0]\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        sample = self.x[index,:,:], self.y[index, :, :]\n",
    "        return sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.length\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37b5c74b",
   "metadata": {},
   "source": [
    "#### 1C) Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "594cdfa1",
   "metadata": {},
   "source": [
    "The model was trained using the Adam optimzer and MSE loss. Validation is conducted every 200 epochs and if the model improves on the validation data, the optimizer and model parameters are saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b3e5469",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( val_data, Multiplexer_model, Student_model, length, training_CHRS, model_output_dim,\n",
    "          optimizer , loss_function, epochs = 10000, num_seqs = 16, batch_size = 16, device = 'cuda'):\n",
    "    \"\"\"\n",
    "    Trains the Multiplexer Model\n",
    "    \n",
    "    Args:\n",
    "        val_data : torch.utils.data.dataloader.DataLoader object\n",
    "            Validation data\n",
    "        \n",
    "        Multiplexer_model: Neural Network object\n",
    "            A multiplexer model that learns to predict variants of the student model\n",
    "        \n",
    "        Student_model: Neural Network object\n",
    "            The model the multiplexer model is training from\n",
    "        \n",
    "        length : int\n",
    "            The number of base-pairs in one generate sequence\n",
    "            \n",
    "        \n",
    "        training_CHRS : array\n",
    "            Set of chromosomes to be considered for training\n",
    "        \n",
    "        model_output_dim : int\n",
    "            The dimension of the output prediction made by the model\n",
    "        \n",
    "        optimizer : pytorch optimizer object\n",
    "            The optimizer function used to train the Multiplexer_model\n",
    "        \n",
    "        loss_function : pytorch loss function object\n",
    "            The loss function used to train the Multiplexer_model\n",
    "        \n",
    "        epochs : int\n",
    "            Number of epochs the Multiplexer is trained on\n",
    "        \n",
    "        num_seqs : int\n",
    "            The number of training samples generated by the method\n",
    "        \n",
    "        batch_size: int\n",
    "            Batch_size used when generating training target from the Student model\n",
    "        \n",
    "        device: 'cpu' or 'cuda'\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    lowest_loss = float(\"inf\")\n",
    "    training_loss = 0\n",
    "    \n",
    "    \n",
    "    for epoch in range(epochs):      \n",
    "        Multiplexer_model.train()\n",
    "        optimizer.zero_grad()\n",
    "        training_loss = 0\n",
    "\n",
    "        x,y = gen_training_data(num_seqs, length, training_CHRS, Student_model, model_output_dim, batch_size, device)\n",
    "        y = y.transpose(1,2)\n",
    "\n",
    "\n",
    "        yhat = Multiplexer_model.forward(x.to(device))\n",
    "        y = torch.reshape(y , (yhat.shape[0], model_output_dim, 4, length)).to(device)\n",
    "        \n",
    "        training_loss = loss_function(yhat, y)\n",
    "        \n",
    "\n",
    "        #Update params\n",
    "        training_loss.backward()\n",
    "        optimizer.step()\n",
    "            \n",
    "        if epoch % 200 == 0:\n",
    "            print(\"Training loss on Epoch \", epoch, \"is \", training_loss.item())\n",
    "\n",
    "            \n",
    "        #validation test    \n",
    "        if epoch % 200 == 0:\n",
    "            Multiplexer_model.eval()\n",
    "            validation_loss = 0\n",
    "            \n",
    "            for x,y in val_data:\n",
    "                with torch.no_grad():\n",
    "\n",
    "\n",
    "                    yhat = Multiplexer_model.forward(x.to(device))\n",
    "                    y = torch.reshape(y , (yhat.shape[0], model_output_dim, 4, length)).to(device)\n",
    "                    validation_loss += loss_function(yhat, y) \n",
    "                    \n",
    "            print(\"Validation loss on epoch \", epoch, \"is \", validation_loss.item())\n",
    "            if validation_loss < lowest_loss:\n",
    "                lowest_loss = validation_loss\n",
    "                ###Uncomment the save methods to save the state_dict and optimizer\n",
    "                #torch.save(model.state_dict(), \"Multiplexer_params.pth\")\n",
    "                #torch.save(optimizer.state_dict(), \"Multiplexer_optim.pth\")\n",
    "               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87e4611e",
   "metadata": {},
   "source": [
    "#### 1D) Final Components"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70e5a8f",
   "metadata": {},
   "source": [
    "After running all of the code above, load in the Genome and Beluga parameters, generate a set of validation data, and start training the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c46171",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "started training\n"
     ]
    }
   ],
   "source": [
    "genome = pyfasta.Fasta('../../data/hg19.fa')\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "##Load in Base model with trained parameters\n",
    "Beluga_model = Beluga().to(device)\n",
    "Beluga_model.load_state_dict(torch.load('../../data/deepsea.beluga.pth'))\n",
    "\n",
    "#define size of validation data\n",
    "val_num_seqs = 8 #number of sequences to use as validation\n",
    "model_output_dim = 2002 #number of features predicted by the length\n",
    "length = 2000 #length of the model input\n",
    "batch_size = 2 #batch_size used to create training and validation data\n",
    "\n",
    "#create training and validation splits\n",
    "training_CHRS = ['chr1', 'chr2', 'chr3', 'chr4', 'chr5', 'chr6', 'chr7','chr10', 'chr11', 'chr12', 'chr13', \n",
    "            'chr14', 'chr15', 'chr16', 'chr17','chr18', 'chr19', 'chr20', 'chr21', 'chr22', 'chrX','chrY']\n",
    "\n",
    "validation_CHRS = ['chr8', 'chr9']\n",
    "\n",
    "\n",
    "#generated validation data\n",
    "val_arr, val_labels_arr = gen_validation_data(val_num_seqs, length, validation_CHRS, Beluga_model, model_output_dim, batch_size, device)\n",
    "validation_data_obj = Multiplexer_Data(val_arr, val_labels_arr)\n",
    "validation_data = DataLoader( validation_data_obj, batch_size)\n",
    "\n",
    "\n",
    "#Define Multiplexer model and training hyper-parameters\n",
    "BM = BelugaMultiplexer().to(device)\n",
    "optimizer = torch.optim.Adam(BM.parameters(), lr = 0.001)  \n",
    "loss_function = nn.MSELoss()\n",
    "epochs = 10\n",
    "num_seqs = 2\n",
    "\n",
    "print(\"started training\")\n",
    "train( validation_data, BM, Beluga_model, length, training_CHRS, model_output_dim,\n",
    "          optimizer , loss_function, epochs , num_seqs , batch_size , device)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e72d3793",
   "metadata": {},
   "source": [
    "The code above serves as a template for training a Multiplexer model. Many features are adjusted and it is recommended that you adapt the code to your specific model. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
